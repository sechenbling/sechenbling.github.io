[
  
    {
    "title": "HDFS ShellCmd&API",
    "url": "/posts/HDFS-ShellCmd&API/",
    "categories": "Bigdata, Hadoop",
    "tags": "hdfs",
    "date": "2024-02-01 22:35:00 +0800",
    





    
    "snippet": "HDFS 命令hadoop fs -xxx和hdfs dfs -xxx 最终效果和命令都是一样的 两个都可以使用hadoop fs -moveFromLocal /local /remote\t从本地剪切粘贴到 HDFShadoop fs -copyFromLocal /local /remote \t从本地文件系统中拷贝文件到 HDFS 路径去hadoop fs -put /local /re...",
    "content": "HDFS 命令hadoop fs -xxx和hdfs dfs -xxx 最终效果和命令都是一样的 两个都可以使用hadoop fs -moveFromLocal /local /remote\t从本地剪切粘贴到 HDFShadoop fs -copyFromLocal /local /remote \t从本地文件系统中拷贝文件到 HDFS 路径去hadoop fs -put /local /remote\t-put：等同于 -copyFromLocalhadoop fs -appendToFile /local /remote\t追加一个文件到已经存在的文件末尾hadoop fs -copyToLocal /remote\t从 HDFS 拷贝到本地hadoop fs -get /remote\t等同于 copyToLocalhadoop fs -ls /remote\t显示目录信息hadoop fs -cat /remote\t显示文件内容hadoop fs -mkdir /remote\t创建路径hadoop fs -cp /remote1 /remote2\t从 HDFS 的一个路径拷贝到 HDFS 的另一个路径hadoop fs -mv /remote1 /remote2\t在 HDFS 目录中移动文件hadoop fs -tail /remote\t显示一个文件的末尾 1kb 的数据hadoop fs -rm /remote\t删除文件hadoop fs -rm -r /remote\t递归删除目录及目录里面内容（文件夹hadoop fs -du\t统计文件夹下每个目录或文件的大小信息hadoop fs -du -s\t统计该文件夹整体大小hadoop fs -du -h\t统计文件夹下每个目录或文件的大小信息 (存在单位换算 B to KB MB GB…)hadoop fs -du -s -h\t统计该文件夹整体大小(存在大小换算hadoop fs -chgrp\thadoop fs -chgrp group 修改组hadoop fs -chmod\thadoop fs -chmod 777 修改三类用户权限hadoop fs -chown\thadoop fs -chown chen:chen 修改用户HDFS 命令@Beforepublic void init() throws IOException, URISyntaxException, InterruptedException{    //连接集群NameNode的地址    URIuri=newURI(\"hdfs://hadoop102:8020\");    //创建一个配置文件    Configurationconfiguration=newConfiguration();    //获取用户信息    Stringuser=\"chen\";    //获取客户端对象    fs=FileSystem.get(uri,configuration,user);    //创建一个文件夹操作    fs.mkdirs(newPath(\"/xiyou/huaguoshan\"));}@Afterpublic void close() throws IOException{    //关闭资源    fs.close();}配置文件中的dfs.replication参数为上传后的副本数，这个参数的生效优先级为代码中的configuration.set(“dfs.replication”,xx) &gt; 项目resource目录下的hdfs-site.xml文件（文件内部增加dfs.replication参数）  &gt;  服务器上的hdfs-site.xml(修改文件内的dfs.replication参数)  &gt; 服务器上的hdfs-default.xml(默认无法修改)"
    },
  
    {
    "title": "HDFS DataNode",
    "url": "/posts/HDFS-DataNode/",
    "categories": "Bigdata, Hadoop",
    "tags": "hdfs",
    "date": "2024-02-01 22:34:00 +0800",
    





    
    "snippet": "DataNode",
    "content": "DataNode"
    },
  
    {
    "title": "Read&Write Process",
    "url": "/posts/HDFS-Read&Write-Process/",
    "categories": "Bigdata, Hadoop",
    "tags": "hdfs",
    "date": "2024-02-01 22:33:00 +0800",
    





    
    "snippet": "写入数据的流程：  客户端创建分布式文件系统DistributedFileSystem  向NameNode请求上传文件          NameNode检查权限      NameNode检查目录结构（目录是否存在）        NameNode响应可以上传文件  客户端请求上传第一个Block(0-128M)，请求NameNode返回DataNode  NameNode返回多个Dat...",
    "content": "写入数据的流程：  客户端创建分布式文件系统DistributedFileSystem  向NameNode请求上传文件          NameNode检查权限      NameNode检查目录结构（目录是否存在）        NameNode响应可以上传文件  客户端请求上传第一个Block(0-128M)，请求NameNode返回DataNode  NameNode返回多个DataNode表示这三个节点存储数据          选择策略(节点距离最近，负载均衡)：                  本地节点          其他机架的一个节点          其他机架的另一个节点                      客户端创建FSDataOutputStream上传数据，向一个DN请求建立Block传输通道，之后再由DN向其他的DN请求建立通道（DN边存储边传输给其他DN），等数据传输完毕进行应答          流的传输单位为packet(64K)                  packet由512B(chunk)+4B(校验位)的chunk累计到64K后形成的          ack队列来接收应答                    网络拓扑-节点距离计算节点之间到共同祖先的距离之和为节点的距离机架感知：副本存储节点的选择假如由三个副本，其中一个副本存储在客户端所在集群 节点上，如果客户端在集群外，则随机选择一个；其他两个副本：一个在另一个机架的随机一个节点，一个在第二个副本所在机架的随机节点。读取数据的流程  客户端创建分布式文件系统对象向NameNode请求下载数据  NameNode判断权限和文件是否存在，如果都满足就将目标文件的元数据信息反馈回去  客户端创建FSDataInputStream选择节点距离最近的节点(首要原则)进行读取（此外还会考虑节点的负载均衡（次要原则）          如果还有其他的块，但当前节点的负债能力已经达到极限了就选择其他的节点进行读取      读取是串行读，先读取第一块，再读取第二块进行拼接      "
    },
  
    {
    "title": "HDFS NameNode&SecondaryNameNode",
    "url": "/posts/HDFS-NameNode&SecondaryNameNode/",
    "categories": "Bigdata, Hadoop",
    "tags": "hdfs",
    "date": "2024-02-01 22:33:00 +0800",
    





    
    "snippet": "      fslmage 镜像文件：存储数据HDFS文件系统元数据的一个永久性的检查点，其中包含HDFS文件系统的所有目录和文件inode的序列化信息。        Edits 记录变化的步骤存放HDFS文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先会被记录到Edits文件当中          seen_txid:记录当前最新的Edits(即最后一个Edits的数字 ...",
    "content": "      fslmage 镜像文件：存储数据HDFS文件系统元数据的一个永久性的检查点，其中包含HDFS文件系统的所有目录和文件inode的序列化信息。        Edits 记录变化的步骤存放HDFS文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先会被记录到Edits文件当中          seen_txid:记录当前最新的Edits(即最后一个Edits的数字      VERSION：记录集群ID      每次NameNode启动的时候都会将Fsimage文件读入内存，加载Edits的更新操作，保证数据是最新的同步的，可以看作NameNode启动的时候就将Fsimage和Edits文件进行了合并  上线后内存的中出现的是fslmage和执行完Edits后的结果          差别在inprogress这个文件 2NN在更新的时候如果有新的操作，会记录在inprogress文件当中      NN 和 2NN 工作机制 思考：NameNode 中的元数据是存储在哪里的？ 首先，我们做个假设，如果存储在 NameNode 节点的磁盘中，因为经常需要进行随机访 问，还有响应客户请求，必然是效率过低。因此，元数据需要存放在内存中。但如果只存在 内存中，一旦断电，元数据丢失，整个集群就无法工作了。因此产生在磁盘中备份元数据的 FsImage。 这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新 FsImage，就会导 致效率过低，但如果不更新，就会发生一致性问题，一旦 NameNode 节点断电，就会产生数 据丢失。因此，引入 Edits 文件（只进行追加操作，效率很高）。每当元数据有更新或者添 加元数据时，修改内存中的元数据并追加到 Edits 中。这样，一旦 NameNode 节点断电，可 以通过 FsImage 和 Edits 的合并，合成元数据。 但是，如果长时间添加数据到 Edits 中，会导致该文件数据过大，效率降低，而且一旦 断电，恢复元数据需要的时间过长。因此，需要定期进行 FsImage 和 Edits 的合并，如果这 个操作由NameNode节点完成，又会效率过低。因此，引入一个新的节点SecondaryNamenode， 专门用于 FsImage 和 Edits 的合并。        第一阶段：NameNode 启动 第一次启动:          NameNode 格式化后，创建 Fsimage 和 Edits 文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。      客户端对元数据进行增删改的请求。      NameNode 记录操作日志，更新滚动日志。      NameNode 在内存中对元数据进行增删改。        第二阶段：Secondary NameNode 工作          Secondary NameNode 询问 NameNode 是否需要 CheckPoint。直接带回 NameNode 是否检查结果。      Secondary NameNode 请求执行 CheckPoint。      NameNode 滚动正在写的 Edits 日志。      将滚动前的编辑日志和镜像文件拷贝到 Secondary NameNode。      Secondary NameNode 加载编辑日志和镜像文件到内存，并合并。      生成新的镜像文件 fsimage.chkpoint。      拷贝 fsimage.chkpoint 到 NameNode。      NameNode 将 fsimage.chkpoint 重新命名成 fsimage。      "
    },
  
    {
    "title": "Hadoop HDFS Structure",
    "url": "/posts/HDFS-Structure/",
    "categories": "Bigdata, Hadoop",
    "tags": "hdfs",
    "date": "2024-02-01 22:31:00 +0800",
    





    
    "snippet": "NameNode：master 主管管理者  管理HDFS的名称空间（原数据）  配置副本策略          设置不同数据的副本的数量        管理数据块的映射信息          比较大的文件分块存储，而且块和块之间的存储没有关系        处理客户端的读写请求DataNode就是slave worker，NameNode下达命令由DataNode执行实际的操作  存储实际...",
    "content": "NameNode：master 主管管理者  管理HDFS的名称空间（原数据）  配置副本策略          设置不同数据的副本的数量        管理数据块的映射信息          比较大的文件分块存储，而且块和块之间的存储没有关系        处理客户端的读写请求DataNode就是slave worker，NameNode下达命令由DataNode执行实际的操作  存储实际的数据块  执行数据块的读写操作SecondaryNameNode 2NN  并非NameNode的热备，当NN挂掉的时候它并不能马上替换NN提供服务  辅助NN分担工作：比如定期合并镜像文件和编辑日志(Fsimage&amp;Edits)并推送给NN  紧急情况下可以恢复部分数据（不是所有数据都给了2NN）Client 客户端  文件切分：文件上传HDFS时，Client将文件切分成一个一个的Block后上传  与NN交互获取文件位置  与DN交互读写数据  提供一些命令来管理HDFS：比如NN格式化  通过一些命令来访问HDFS：进行增删改查文件块大小：只是指占用上限，如果一个文件小于128M其占用磁盘就是其本身  Hadoop2，3是128M  Hadoop1是64M  找到块的时间是传输块时间的1%最佳  块大小设置          块太小，会增加寻址时间，程序一直在找块的开始位置（块数量多了）      块太大，从磁盘的传输时间会明显大于定位这个块开始位置所需的时间，在处理这块数据时会非常慢      块大小设置:主要取决于磁盘传输速率      "
    },
  
    {
    "title": "Hadoop Command",
    "url": "/posts/Hadoop-Command/",
    "categories": "Bigdata, Hadoop",
    "tags": "hadoop",
    "date": "2024-02-01 22:30:00 +0800",
    





    
    "snippet": "hadoop的常用命令启动hadoop所有进程start-all.sh 等价于 start-dfs.sh + start-yarn.sh但是一般不推荐使用start-all.sh(因为开源框架中内部命令启动有很多问题)。单进程启动。sbin/start-dfs.shsbin/hadoop-daemons.sh --config .. --hostname .. start namenode ...",
    "content": "hadoop的常用命令启动hadoop所有进程start-all.sh 等价于 start-dfs.sh + start-yarn.sh但是一般不推荐使用start-all.sh(因为开源框架中内部命令启动有很多问题)。单进程启动。sbin/start-dfs.shsbin/hadoop-daemons.sh --config .. --hostname .. start namenode ...sbin/hadoop-daemons.sh --config .. --hostname .. start datanode ...sbin/hadoop-daemons.sh --config .. --hostname .. start sescondarynamenode ...sbin/hadoop-daemons.sh --config .. --hostname .. start zkfc ...         //sbin/start-yarn.shlibexec/yarn-config.shsbin/yarn-daemon.sh --config $YARN_CONF_DIR  start resourcemanagersbin/yarn-daemons.sh  --config $YARN_CONF_DIR  start nodemanager常用命令查看指定目录下内容hdfs dfs –ls [文件目录]hdfs dfs -ls -R   //显式目录结构eg:hdfs dfs –ls /user/lynn/打开某个已存在文件hdfs dfs –cat [file_path]eg:hdfs dfs -cat /user/wangkai.pt/data.txt将本地文件存储至hadoophdfs dfs –put [本地地址] [hadoop目录]hdfs dfs –put /home/t/file.txt  /user/t将本地文件夹存储至hadoophdfs dfs –put [本地目录] [hadoop目录]hdfs dfs –put /home/t/dir_name /user/t将hadoop上某个文件down至本地已有目录下hadoop dfs -get [文件目录] [本地目录]hadoop dfs –get /user/t/ok.txt /home/t删除hadoop上指定文件hdfs  dfs –rm [文件地址]hdfs dfs –rm /user/t/ok.txt删除hadoop上指定文件夹(包含子目录等)hdfs dfs –rm [目录地址]hdfs dfs –rmr /user/t在hadoop指定目录内创建新目录hdfs dfs –mkdir /user/thdfs  dfs -mkdir - p /user/centos/hadoop在hadoop指定目录下新建一个空文件hdfs dfs  -touchz  /user/new.txt将hadoop上某个文件重命名\thdfs dfs –mv  /user/test.txt  /user/ok.txt  (将test.txt重命名为ok.txt)将hadoop指定目录下所有内容保存为一个文件，同时down至本地hdfs dfs –getmerge /user /home/t将正在运行的hadoop作业kill掉hadoop job –kill  [job-id]查看帮助hdfs dfs -help安全模式退出安全模式NameNode在启动时会自动进入安全模式。安全模式是NameNode的一种状态，在这个阶段，文件系统不允许有任何修改。系统显示Name node in safe mode，说明系统正处于安全模式，这时只需要等待十几秒即可，也可通过下面的命令退出安全模式：/usr/local/hadoop$bin/hadoop dfsadmin -safemode leave进入安全模式在必要情况下，可以通过以下命令把HDFS置于安全模式：/usr/local/hadoop$bin/hadoop dfsadmin -safemode enter节点添加添加一个新的DataNode节点，先在新加节点上安装好Hadoop，要和NameNode使用相同的配置(可以直接从NameNode复制)，修改HADOOPHOME/conf/master文件，加入NameNode主机名。然后在NameNode节点上修改HADOOPHOME/conf/master文件，加入NameNode主机名。然后在NameNode节点上修改HADOOP_HOME/conf/slaves文件，加入新节点名，再建立新加节点无密码的SSH连接，运行启动命令为：/usr/local/hadoop$bin/start-all.sh负载均衡HDFS的数据在各个DataNode中的分布可能很不均匀，尤其是在DataNode节点出现故障或新增DataNode节点时。新增数据块时NameNode对DataNode节点的选择策略也有可能导致数据块分布不均匀。用户可以使用命令重新平衡DataNode上的数据块的分布：/usr/local/hadoop$bin/start-balancer.sh快速查询补充hadoop常用命令：hdfs dfs  查看Hadoop HDFS支持的所有命令hdfs dfs –ls  列出目录及文件信息hdfs dfs –lsr  循环列出目录、子目录及文件信息hdfs dfs –put test.txt /user/sunlightcs  将本地文件系统的test.txt复制到HDFS文件系统的/user/sunlightcs目录下hdfs dfs –get /user/sunlightcs/test.txt .  将HDFS中的test.txt复制到本地文件系统中，与-put命令相反hdfs dfs –cat /user/sunlightcs/test.txt  查看HDFS文件系统里test.txt的内容hdfs dfs –tail /user/sunlightcs/test.txt  查看最后1KB的内容hdfs dfs –rm /user/sunlightcs/test.txt  从HDFS文件系统删除test.txt文件，rm命令也可以删除空目录hdfs dfs –rmr /user/sunlightcs  删除/user/sunlightcs目录以及所有子目录hdfs dfs –copyFromLocal test.txt /user/sunlightcs/test.txt  从本地文件系统复制文件到HDFS文件系统，等同于put命令hdfs dfs –copyToLocal /user/sunlightcs/test.txt test.txt  从HDFS文件系统复制文件到本地文件系统，等同于get命令hdfs dfs –chgrp [-R] /user/sunlightcs  修改HDFS系统中/user/sunlightcs目录所属群组，选项-R递归执行，跟linux命令一样hdfs dfs –chown [-R] /user/sunlightcs  修改HDFS系统中/user/sunlightcs目录拥有者，选项-R递归执行hdfs dfs –chmod [-R] MODE /user/sunlightcs  修改HDFS系统中/user/sunlightcs目录权限，MODE可以为相应权限的3位数或+/-{rwx}，选项-R递归执行hdfs dfs –count [-q] PATH  查看PATH目录下，子目录数、文件数、文件大小、文件名/目录名hdfs dfs –cp SRC [SRC …] DST       将文件从SRC复制到DST，如果指定了多个SRC，则DST必须为一个目录hdfs dfs –du PATH  显示该目录中每个文件或目录的大小hdfs dfs –dus PATH  类似于du，PATH为目录时，会显示该目录的总大小hdfs dfs –expunge  清空回收站，文件被删除时，它首先会移到临时目录.Trash/中，当超过延迟时间之后，文件才会被永久删除hdfs dfs –getmerge SRC [SRC …] LOCALDST [addnl]      获取由SRC指定的所有文件，将它们合并为单个文件，并写入本地文件系统中的LOCALDST，选项addnl将在每个文件的末尾处加上一个换行符hdfs dfs –touchz PATH   创建长度为0的空文件hdfs dfs –test –[ezd] PATH     对PATH进行如下类型的检查：  -e PATH是否存在，如果PATH存在，返回0，否则返回1  -z 文件是否为空，如果长度为0，返回0，否则返回1  -d 是否为目录，如果PATH为目录，返回0，否则返回1hdfs dfs –text PATH  显示文件的内容，当文件为文本文件时，等同于cat，文件为压缩格式(gzip以及hadoop的二进制序列文件格式)时，会先解压缩    hdfs dfs –help ls  查看某个[ls]命令的帮助文档"
    },
  
    {
    "title": "Data Warehouse Design",
    "url": "/posts/Data-Warehouse-Design/",
    "categories": "Bigdata, DataWarehouse",
    "tags": "data warehouse",
    "date": "2023-01-28 22:00:00 +0800",
    





    
    "snippet": "数据仓库设计数据仓库分层规划  使数据体系更加清洗，简单化复杂问题（one data理论分层）    ODS:  operation data store:业务系统的原始数据，未经处理的数据，放在HDFS以文件形式的存储，从HDFS原路径load(底层是剪切操作)到ODS层。  DWD:  data warehouse detail:基于维度建模理论进行构建，保存各业务过程中最小粒度的操作记...",
    "content": "数据仓库设计数据仓库分层规划  使数据体系更加清洗，简单化复杂问题（one data理论分层）    ODS:  operation data store:业务系统的原始数据，未经处理的数据，放在HDFS以文件形式的存储，从HDFS原路径load(底层是剪切操作)到ODS层。  DWD:  data warehouse detail:基于维度建模理论进行构建，保存各业务过程中最小粒度的操作记录，存放事实表。  DIM:  dimension:基于维度建模理论进行构建，存放维度表，保存一致性维度信息。  DWS:  data warehouse summary:需求驱动来建立对应的表，以分析的主题对象作为建模驱动构建公共统计粒度（不同需求之间通用的计算结果，减少重复计算）的汇总表。  ADS:  application data service:存放各项统计指标结果。数仓构建流程数据调研：  业务调研：熟悉业务流程，熟悉业务数据  需求分析：明确需求所需的业务过程及维度明确数据域按照特定标准对数据进行纵向切分，划分数据域便于数据的管理和应用  比如电商业务过程中将业务分为交易域，流量域，用户域，互动域等多个域  说白其实对应DWD层的事实表，DWS的汇总表来自事实表，其实也来自数据域            数据域      业务过程                  交易域      加购、下单、取消订单、支付成功、退单、退款成功              流量域      页面浏览、启动应用、动作、曝光、错误              用户域      注册、登录              互动域      收藏、评价              工具域      优惠券领取、优惠券使用（下单）、优惠券使用（支付）      构建业务总线矩阵服务于维度模型的设计    总线矩阵中通常只包含事务型事实表，其他两种类型的表单独设计维度模型设计参照业务总线矩阵，构建事实表存储在DWD层，维度表存储在DIM层明确统一指标整理出指标体系理论，包括原子指标、派生指标、衍生指标，主要是为了指标定义标准化，遵循一套标准，避免重复定义和定义歧义问题  原子指标：基于某一业务过程的度量值，原子指标核心功能就是对指标的聚合逻辑进行定义          比如每个订单都有金额，订单总额就是一个原子指标      原子指标只是用来辅助下面两个定义的一个概念，通常没有实际的对应需求        派生指标：基于原子指标，通常会对应实际的统计需求，关系图如下    衍生指标：在多个派生指标的基础上，通过各种逻辑运算复合而来的，比如比率、比例等    指标理论体系意义：公共的派生指标保存在DWS层，统计需求足够多的时候，会出现公共的派生指标，因此DWS层设计可以参考现有的统计需求整理出来汇总模型设计一张汇总表会包括业务过程相同，统计周期相同，统计粒度相同的多个派生指标    汇总表是由一张事实表（一个业务过程）汇总而来的，汇总表却不止一张，属于一对多的关系数据仓库环境配置绝大多数与事实表的业务表都是增量同步，绝大多数与维度表相关的业务表都是全量同步（非绝对，比如需要设计成拉链表类型的维度表不需要同步所有数据，只需要同步新增变化，用增量同步即可）数仓运行环境准备  Hive环境：引擎变更，从MR转变为使用Spark          区分                  Hive on Spark（语法是Hive语法，执行引擎变为Spark）；Spark采用RDD执行。除了数仓的建模和SQL以外对数据安全和认证以及元数据管理的周边数仓任务框架支持更好，生态更好。                          Spark on Hive（语法是Spark，Spark负责SQL解析，Hive作为存储元数据）；Spark采用RDD执行。效率更高但是生态较差。              两者都可以作为数仓运行环境；                                记得要重新编译Hive的源码，使其内部能够使用Spark3.0.0，在Hive部署的节点上部署Spark          Hive on spark是以一个会话作为一个资源分配，一个会话为每次开启的Hive 相较于使用MR每个mr程序一个资源分配，开启会话的时候都比较忙，但是后续操作spark引擎会快于mr引擎                      Yarn环境：增加Application Master资源比例，虚拟机集群环境资源较少，默认只分配10%资源给一个Job会导致同一时刻只能运行一个Job的情况（资源总量太少，10%就更少了，内存资源可能只够一个任务运行），适当调大AM可以使用的资源比例使其使用更多资源    &lt;property&gt;  &lt;name&gt;yarn.scheduler.capacity.maximum-am-resource-percent&lt;/name&gt;  &lt;value&gt;0.1&lt;/value&gt; &lt;!-- 修改为80% --&gt;  &lt;description&gt;      Maximum percent of resources in the cluster which can be used to run application masters i.e. controls number of concurrent running applications.  &lt;/description&gt;&lt;/property&gt;      数仓开发环境准备  使用DBeaver或者DataGrip，两者都需要使用JDBC协议连接到Hive，所以需要启动HiveServer2服务(Hiveserver2就是用于提供JDBC接口)，使用DataGrip连接到Hive即可。"
    },
  
    {
    "title": "Data Warehouse Model",
    "url": "/posts/Data-Warehouse-Model/",
    "categories": "Bigdata, DataWarehouse",
    "tags": "data warehouse",
    "date": "2023-01-28 21:30:00 +0800",
    





    
    "snippet": "数据仓库介绍数据仓库  用于存储分析报告的数据系统          并不产生数据（数据来自于外部系统） 也不消费数据（其结果开放给外部应用使用）        构建面向分析的集成化数据环境，分析结构提供决策  对比OLAP和OLTP          各类型的数据库保存相关数据（关系型数据库是OLTP的典型应用） 但不会在数据库上直接分析数据      数据仓库是OLAP的一种实现 （OL...",
    "content": "数据仓库介绍数据仓库  用于存储分析报告的数据系统          并不产生数据（数据来自于外部系统） 也不消费数据（其结果开放给外部应用使用）        构建面向分析的集成化数据环境，分析结构提供决策  对比OLAP和OLTP          各类型的数据库保存相关数据（关系型数据库是OLTP的典型应用） 但不会在数据库上直接分析数据      数据仓库是OLAP的一种实现 （OLAP 联机分析处理系统 面向分析 支持分析）      数据仓库特点  面向主题subject oriented 主题抽象概念，在较高层次上数据综合、归类并分析利用的抽象  集成性integrated 主题相关的数据通常会分布在多个操作系统中，彼此分散独立异构，需要集成在数仓主题下          数据进入数据仓库前需要经过统一和综合，对数据进行抽取、清理、转换和汇总                  ETL：抽取 转换 加载                    统一数据源中的矛盾之处：同名异义 字长不统一等      进行数据综合和计算        非易失性non-volatile 分析数据而非创造数据          对数仓中的数据操作大多是查询或者数据挖掘，一般会保留较长时间      查询操作较多，修改和删除操作较少        时变性time-variant 数据仓库的数据随着时间更新以适应决策的需要          离线分析：需要随着时间更新以适应决策的需要      数仓开发语言SQL结构化查询语言：分析领域主流开发语言，SQL本身是针对数据库软件设计的，但是在大数据数仓领域，很多数仓都会支持SQL语法数据仓库建模仓库流程图数据建模数据组织和存储方法与数据的处理（建表+写SQL）  每一层的数据是具有依赖关系的，需要按链路流程下去  实时处理：流处理 stream  离线处理：批处理 batch(一般按照时间为单位来作为一批处理数据)  用途          高性能：良好的数据模型能够帮助我们快速查询所需要的数据。      低成本：良好的数据模型能减少重复计算，实现计算结果的复用，降低计算成本。      高效率：良好的数据模型能极大的改善用户使用数据的体验，提高使用数据的效率。      高质量：良好的数据模型能改善数据统计口径的混乱，减少计算错误的可能性。      数据建模工作流每个工作流程之间是具有前后依赖关系的，需要保证前面的任务处理完才可以处理后续的任务，对每个工作流程的脚本需要控制对应的开启时间，可能存在时间错位，需要使用对应的调度工具来确保流程能够按顺序正常执行（只需指定开启的时间，而不需要指定每一步的时间）数据仓库建模方法  ER模型（并不是那么适合用于数据分析）：          将复杂的数据抽象为两个概念，实体和关系，使用一系列范式设计数据库（通常是关系型数据库），减少数据冗余，增强数据一致性。当然一定的数据冗余可以提高查询的速率，提高性能。范式提高：数据冗余减少，数据一致性增强，表数量增多，查询性能降低。      三范式：                  第一范式：属性不可再切分          第二范式：表中不存在不分函数依赖          第三范式：表中不存在传递函数依赖                    对简单的需求可能需要跨越大量的表进行join操作才能获取到相应的数据，比较影响性能，并不适合直接用于分析统计操作。        维度模型          维度模型将业务通过事实和维度两个概念进行呈现，事实对应业务过程，而维度通常对应业务过程发生时所处的环境。抽象为事实表和维度表。关注于数据分析，如何更快完成需求分析和大规模复杂查询的相应性能。只需要通过一个事实表和其他维度表进行关联即可，使用维度表里的字段进行过滤。      缺点在于数据冗余，比如Date中的年分，同一年中多天都是一个年份，但是放在大数据集群中对存储空间并不考虑，效率更重要（HDFS无限扩展）；此外更改数据时会出现数据一致性问题，不过数仓不会针对一条或几条数据进行更改，而是批量大规模更改。      业务过程可以概括为一个个不可拆分的行为事件，比如商品下单等。      维度模型事实表包含与业务过程有关的维度引用（维度表外键）以及该业务过程的度量（可累加的数字类型字段）  分类：          事务事实表：用于记录业务过程，保存的是各业务过程的原子操作，即最细粒度（每一条最小的操作记录）的操作事件。粒度是指事实表中一行数据表达的业务细节程度。                  设计过程：                          选择业务过程：一个业务过程对应一张事务事实表，确定需要建立哪些事实表（需求驱动）              声明粒度：定义事实表每行数据具体含义并尽可能确定最细粒度              确认维度：尽可能多的确定与每张事实表相关的维度（要与业务逻辑相关联），维度越丰富，维度模型所能支持的指标更丰富              确认事实：事实指每个业务过程的度量值（比如次数，个数等概念）                                不足之处：                          存量型指标：得到某个结果，需要涉及到两张事务事实表的结果进行结合才可以获得，需要对两张表的全表数据进行聚合才能得到结果，但是事务型事实表行多，效率较低。（非join操作关联表，指的是一个需求需要汇总一个或多个全表的数据，一个表行多数据多）              多事务关联统计：需要将多个事务型事实表进行join关联才能获取结果的情况，事实表行很多，join操作影响效率。                                          周期快照事实表：以规律性，可预见的时间间隔来记录事实，用于分析一些存量型或者状态型指标                  对于存量型指标，只需定期同步一份数据到数仓后即可构建周期性快照事实表，基本都是针对于全量表进行这些操作。          对于连续型指标（比如运动速度等连续的值），无法捕获其变动的原子事务操作，只能定期采样构建相关的周期型快照事实表。          同一个业务过程可以同时有事务型事实表和周期性快照事实表，两者并不冲突，是为了解决不同的问题，是一个互补的过程。          设计过程：                          确定粒度：粒度由采样周期（取决于需求）和维度描述（根据统计指标决定）                                  每日每个仓库中的每个商品的库存，粒度为每日-仓库-库存                                            确认事实：根据统计指标决定，上例的事实为商品库存                                周期快照事实类型：                          可加事实：可以按照事实表相关的所有维度进行相加，比如事务性事实表的事实              半可加事实：只有一部分维度可以进行累加，比如周期型快照事实表，例子为上例的时间维度，相加没有意义，另外两个维度相加均有一定意义。              不可加事实：完全不具备可加性：比如比率型事实，通常可以转化为可加事实，比如将比率转化为分子和分母，比如退货率=退货次数/下单次数 分为两个可加事实                                          累计快照事实表：基于一个业务流程中的多个关键业务过程联合处理而构成的事实表：比如交易流程中的下单支付发货收获过程。表中的每一条数据都会根据流程逐渐更新。比如交易流程中各个过程的时间。                  设计过程：多个业务过程对应一张累计型快照事实表          其余过程基本同事务型事实表                    维度表主要包括一个主键和各种维度字段，维度字段称为维度属性  设计步骤：          确定维度：两个方面，第一每个与事实表相关的维度都有一个维度表，也存在多个事实和一个维度表相关的情况，这个时候需要保证维度表的唯一性；第二如果维度属性比较少，可以直接把维度表信息放入事实表中，称为维度退化，用于减少join操作。      确定主维表和相关维表：主维表和相关维表都是指业务系统中与某些维度相关的表，比如商品维度衍生出来的其他与商品相关的信息，维度表的粒度通常与主维表相同。行与主维表一致，列来自相关维表。      确定维度属性：维度属性主要来自于主维表和相关维表，也可以通过进一步处理衍生成一个维度属性                  维度属性确定需要以下需求：                          尽可能丰富维度属性：维度属性是分析统计时约束条件和分组字段的基本来源，是数据易用性的关键，维度属性的丰富直接影响能够支持的指标的丰富程度。              尽量不适用编码，而是使用明确的文字说明，一般可以编码和文字共存                                  比如MySQL中对于支付方式，存储的是代表为支付宝或者微信的编码，而不是直接存储支付宝或者微信这些文字，并生成对应的编码表，但是在维度表里面，尽可能使用明确的文字说明或者编码与文字共存                                            尽量沉淀出通用的维度属性              有些维度属性的获取需要比较复杂的逻辑处理，例如多个字段拼接处理，为了避免重复处理，可以将相关的维度属性沉淀到维度表中。                                            维度设计要点          规范化和反规范化：                  规范化：减少数据冗余增强数据一致性          反规范化：将多张表的数据冗余到一张表，目的是减少join操作提高查询性能          数据仓库系统的主要目的是用于数据分析和统计，所以是否方便用户进行统计分析决定了模型的优劣。采用雪花模型，用户在统计分析的过程中需要大量的关联操作，使用复杂度高，同时查询性能很差，而采用星型模型，则方便、易用且性能好。所以出于易用性和性能的考虑，维度表一般是很不规范化的。                      维度变化：维度属性通常不是静态的，是会随着时间变化的，数仓一个重要特点就是反应历史的变化，所以需要保存维度的历史状态。          全量快照表：一定固定时间保存一份全量的维度数据，简单有效，但是浪费存储空间      拉链表：更加高效的保存维度信息的历史状态，记录每条记录的生命周期，一旦一条记录的生命周期结束，就重新开始一条新的记录，并把当前日期放入生效开始日期，如果生命周期未结束，采用特殊值进行标记即可。拉链表比较适合数据会发生变化但是变化频率不高的维度表，频繁变化的维度还是全量快照表更好。        多值维度：事实表中一条记录对应维度表中的多条记录，比如订单事实表中的一条订单对应商品维度表中的多个商品，针对这种情况的解决方法如下：          降低事实表的粒度（建议用）：例如将订单事实表的粒度由一个订单降低为一个订单中的一个商品项      在事实表中设计多个维度值字段，每个字段保存一个维度id，这种方案只适合多值维度个数固定的情况        多值属性：维度表中的某个属性同时拥有多个值，但是多个值字段针对不同商品无法形容，比如手机的系统属性，而食物没有系统这个属性的情况          （通用方式）将多值属性放到一个字段当中，以键值对(k1:v1,k2:v2)的形式存储      将多值属性放到多个字段，每个字段对应一个属性，这种方案只适用于多值属性个数固定的情况。      "
    },
  
    {
    "title": "Data Warehouse DWS&ADS Develop",
    "url": "/posts/Data-Warehouse-DWS&ADS-Develop/",
    "categories": "Bigdata, DataWarehouse",
    "tags": "data warehouse",
    "date": "2023-01-27 13:30:00 +0800",
    





    
    "snippet": "DWS层汇总数据层，由明细数据汇总而来的数据，基于上层的指标需求，构建公用的中间的统计结果，为了减少重复计算。设计过程  DWS层的设计参考指标体系。          原子指标，派生指标，衍生指标        DWS层的数据存储格式为orc列式存储+snappy压缩。  DWS层表名的命名规范为dws_数据域_统计粒度_业务过程_统计周期（1d/nd/td）注：1d表示最近1日，nd表示...",
    "content": "DWS层汇总数据层，由明细数据汇总而来的数据，基于上层的指标需求，构建公用的中间的统计结果，为了减少重复计算。设计过程  DWS层的设计参考指标体系。          原子指标，派生指标，衍生指标        DWS层的数据存储格式为orc列式存储+snappy压缩。  DWS层表名的命名规范为dws_数据域_统计粒度_业务过程_统计周期（1d/nd/td）注：1d表示最近1日，nd表示最近n日，td表示历史至今。          统计粒度：通俗点就是表里的用来分组的字段（字段数并不固定）      ADS层根据具体业务需求来获取对应的数据，存放各项统计指标标准开窗函数：区分开窗函数和聚合函数  聚合函数：按照字段进行分组(group by)进行shuffle，将相同码的数据进行聚合，然后得到每一组的KV键值（多行聚合一行）  开窗函数：和聚合函数很类似，按照partition by 的字段进行shuffle，将相同key的数据都缓存起来（多行“聚合”多行），前一个多行是可以人为指定大小的，后一个多行值得是对筛选数据进行窗口后的多个结果  补充：这些窗口的划分都是在分区内部！超过分区大小就无效了，可以看到如果不指定ROWS BETWEEN,默认统计窗口为从起点到当前行;关键是理解 ROWS BETWEEN 含义,也叫做window子句：          PRECEDING：往前      FOLLOWING：往后      CURRENT ROW：当前行      UNBOUNDED：无边界      UNBOUNDED PRECEDING 表示从最前面的起点开始      UNBOUNDED FOLLOWING：表示到最后面的终点      CREATE TABLE student_scores(    id int,    studentId int,    language int,    math int,    english int,    classId string,    departmentId string);INSERT INTO table student_scores VALUES  (1,111,68,69,90,'class1','department1'),  (2,112,73,80,96,'class1','department1'),  (3,113,90,74,75,'class1','department1'),  (4,114,89,94,93,'class1','department1'),  (5,115,99,93,89,'class1','department1'),  (6,121,96,74,79,'class2','department1'),  (7,122,89,86,85,'class2','department1'),  (8,123,70,78,61,'class2','department1'),  (9,124,76,70,76,'class2','department1'),  (10,211,89,93,60,'class1','department2'),  (11,212,76,83,75,'class1','department2'),  (12,213,71,94,90,'class1','department2'),  (13,214,94,94,66,'class1','department2'),  (14,215,84,82,73,'class1','department2'),  (15,216,85,74,93,'class1','department2'),  (16,221,77,99,61,'class2','department2'),  (17,222,80,78,96,'class2','department2'),  (18,223,79,74,96,'class2','department2'),  (19,224,75,80,78,'class2','department2'),  (20,225,82,85,63,'class2','department2');SELECT studentId,math,departmentId,classId,  count(math) over() as count1,  count(math) over(partition by classId) as count2,  count(math) over(partition by classId order by math) as count3,  count(math) over(partition by classId order by math rows between 1 preceding and 2 following) as count4FROM student_scores WHERE departmentId='department1';studentid   math    departmentid    classid count1  count2  count3  count4111         69      department1     class1  9       5       1       3113         74      department1     class1  9       5       2       4112         80      department1     class1  9       5       3       4115         93      department1     class1  9       5       4       3114         94      department1     class1  9       5       5       2124         70      department1     class2  9       4       1       3121         74      department1     class2  9       4       2       4123         78      department1     class2  9       4       3       3122         86      department1     class2  9       4       4       2"
    },
  
    {
    "title": "Data Warehouse DWD Develop",
    "url": "/posts/Data-Warehouse-DWD-Develop/",
    "categories": "Bigdata, DataWarehouse",
    "tags": "data warehouse",
    "date": "2023-01-27 11:30:00 +0800",
    





    
    "snippet": "DWD层DWD层设计要点  DWD层的设计依据是维度建模理论，该层存储维度模型的维度表（重点）  DWD层的数据存储格式为orc列式存储+snappy压缩          DWD层表名的命名规范为dwd_数据域_表名_单分区全量增量标识（full/inc）      数据域：对数据进行纵向的分类，方便数据管理和使用，每一类称为一个域，通常是按照业务过程来划分数据域，业务过程其实就是事实表，...",
    "content": "DWD层DWD层设计要点  DWD层的设计依据是维度建模理论，该层存储维度模型的维度表（重点）  DWD层的数据存储格式为orc列式存储+snappy压缩          DWD层表名的命名规范为dwd_数据域_表名_单分区全量增量标识（full/inc）      数据域：对数据进行纵向的分类，方便数据管理和使用，每一类称为一个域，通常是按照业务过程来划分数据域，业务过程其实就是事实表，其实就是对事实表进行划分。      事实表设计：  根据业务总线矩阵（确认业务，声明粒度，确认维度，确认事实），明确业务过程和哪个维度有关系并不是需求决定的，而是根据数仓的具体业务逻辑决定的，维度的确定是比较灵活的，尽可能多的确定与业务过程相关的维度即可。  增量分区都是每天一个分区，每个分区存放当天新增的数据（增量表逻辑相同）  首日假设根据之前数据都是一次操作，根据数据中的字段进行按照数据内的字段动态分区  后续的数据按照具体当天的时间分区放即可  每个数据域的事实表总共写两个SQLHive时间戳转换  from_unixtime(bigint,time_format)，固定从unix epoch(1970-01-01)零时区计时  from_utc_timestamp(ts,时区):          东八区为’GMT+08’      ts为double时单位为秒，为int时单位为毫秒，要注意类型看是否需要进行单位转换      这个函数只是转换ts的数值，如果想要输出yyyy-MM-dd的格式，需要在外层套date_format(from_utc_timestamp,time_format)      "
    },
  
    {
    "title": "Data Warehouse DIM Develop",
    "url": "/posts/Data-Warehouse-DIM-Develop/",
    "categories": "Bigdata, DataWarehouse",
    "tags": "data warehouse",
    "date": "2023-01-27 10:30:00 +0800",
    





    
    "snippet": "DIM层DIM层设计要点  DIM层的设计一句是维度建模理论，该层存储维度模型的维度表（重点）  DIM层的数据存储格式为orc列式存储+snappy压缩（相较于ODS的存储格式和压缩更快）          ODS在乎的是减少存储空间，其他层在乎的是查询更快        DIM层表名的命名规范为dim_表名_全量表或者拉链表标识（full/zip）维度表设计：  确定维度表  确定主维度...",
    "content": "DIM层DIM层设计要点  DIM层的设计一句是维度建模理论，该层存储维度模型的维度表（重点）  DIM层的数据存储格式为orc列式存储+snappy压缩（相较于ODS的存储格式和压缩更快）          ODS在乎的是减少存储空间，其他层在乎的是查询更快        DIM层表名的命名规范为dim_表名_全量表或者拉链表标识（full/zip）维度表设计：  确定维度表  确定主维度表和相关维度表  确定维度属性          尽可能生成丰富的维度属性      尽量不使用编码而使用文字说明或者编码加文字说明      沉淀出通用的维度属性      商品维度表                  可以参考原始数据在MySQL中的表，找到商品MySQL表并找到与其相关联的其他MySQL表，考虑是否和维度表有关，在考虑相关的Hive维度表如何设计。          全量表数据，数据装载从ODS的一个分区到DIM层的一个分区，时间分区要对应上                    DIM层 dev experienceHive复杂数据类型构建  array：          array() 一行一出: array(val1,val2…)      collect_set() 多行一出        map:          map():map(key1,val1,key2,val2…)      str_to_map(text,delimiter1,delimiter2):                  delimiter1用于分割k-v键值对，默认值为 ‘,’          delimiter2用户分割k和v,默认值为’:’                      struct:          struct(val1,val2,val3…):values为表的column      named_struct(name1,val1,name2,val2…):同时给出field的name和val      CTE语法common table expression公共表表达式  原来一个表自己join自己需要写出两遍一样的表，提前声明子查询的表只写一遍供其他公共部分使用语法          with table_name as(select_expression) 这就声明好了一个子查询并写出，可以重复引用这个table_name      想要一次声明多个不同的子查询，语法如下: 其中with只需要写一遍        WITH    table_name1 as(select_expression1),    table_name2 as(select_expression2),    table_namen as(select_expressionn)SELECT xxx                    DIM层 dev question  表信息保存          表中数据需要以orc文件形式保存，而原始数据只有txt文件或者其他类型的文件，可以先创建一个tmp的临时表（默认指定为TEXTFILE存储格式，或者指定成其他对应类型的文件），然后通过insert语句将临时表的数据写入到最终的成表内部解决因为表数据存储格式不同带来的读取问题。      例子：数据保存在txt文件当中，但是表指定stored as orc，如果直接将txt存储到表路径下，最后读取的数据会因为文件格式出错，此时可以创建一个tmp临时表，将txt数据保存到临时表路径下，然后通过insert overwrite into table select * from tmp_table，hive底层会将存储格式进行转化将文件转存为orc格式                  原理：Hive底层建表时都会解析成对应的InputFormat和OutputFormat，利用TextInputFormat从临时数据表读取数据，并通过OrcOutputFormat写入到指定表中完成文件格式的转换          如何查看一个表的具体信息：             SHOW CREATE TABLE table_name                                 可以查看一个表的各种具体信息，包括各类InputFormat，OutputFormat                      处理敏感信息          表中存在部分敏感信息，比如用户具体名称和联系方式等数据，可以在数据装载的时候使用Hive内部内置的加密函数进行脱敏处理:md5(data)        Hive动态分区          通过数据自身特性通过一个insert语句将不同的数据写到不同的分区内      将所有结果集union合并以下（不同行），然后利用动态分区将数据写到不同的分区内        处理拉链表类型的增量表数据          首日和其他时间的处理逻辑是不一样的                  首日：全量表为最初始的用户表，初始的拉链表就是最开始的全量表          其他时间：根据表的操作时间（binlog等方式）得到用户的变化表，然后再将变化表与之前的拉链表进行合并得到新的拉链表                          合并：取出之前拉链表中没有变化的数据，变化的数据取出放入过期分区后将变化后的数据和变化新增的数据与没有变化的数据合并成新的分区表                                          具体操作SQL流程如下：                  将昨日的旧表取出，并于今日新增的表进行全外连接，这个操作会在两个表某一字段（一般是所谓的主码概念）上进行，分为以下几种情况                          旧表有的数据，而新表没有，将这部分旧表有的数据直接保留下来              旧表没有的数据，而新表有的数据，将这部分新的数据直接追加到表中              旧表和新表都有的数据，说明发生了update操作，需要根据具体的业务逻辑确认需要保留新旧表的哪些字段。                                将上一步中三种情况获取的全外连接表保留下来，供下一步insert装载数据使用，可以使用CTE语法保留这张表。          使用动态分区的手段，通过一条insert将数据分隔开，比如如果通过时间进行分区，可以在insert代码后加上partition(时间)，时间这个字段必须在全外连接的表中存在。                    "
    },
  
    {
    "title": "Data Warehouse ODS Develop",
    "url": "/posts/Data-Warehouse-ODS-Develop/",
    "categories": "Bigdata, DataWarehouse",
    "tags": "data warehouse",
    "date": "2023-01-27 09:30:00 +0800",
    





    
    "snippet": "ODS层  表结构设计依托于从业务系统同步过来的数据结构  ODS层要保存全部的历史数据，数据量较大，使用压缩程度较高的格式gzip  ODS层的命名规范为ods_表名_单分区增量全量标识(inc或者full)Hive数仓启动  启动Hive服务    nohup ./hive --service metastore &amp;nohup ./hive --service hiveserve...",
    "content": "ODS层  表结构设计依托于从业务系统同步过来的数据结构  ODS层要保存全部的历史数据，数据量较大，使用压缩程度较高的格式gzip  ODS层的命名规范为ods_表名_单分区增量全量标识(inc或者full)Hive数仓启动  启动Hive服务    nohup ./hive --service metastore &amp;nohup ./hive --service hiveserver2 &amp;        HiveServer的相关问题          OOM问题（导致连接断开或者SQL语句无法运行）：修改conf/hive-env.sh中的HADOOP_HEAPSIZE大小        Hive相关问题：          MySQL中的空值定义（null）与Hive的空值定义(\\N)不一样，且DataX并没有提供将MySQL的null值转化为空值的功能，为了保证同步的数据能够正常被Hive解析，可以在建表的时候指定将空值存为空字符串 NULL DEFINED AS’‘。但是反过来从HDFS导到MySQL可以自定义空值形式。      ODS层表  建表：          日志表：分为启动日志和具体操作日志，可以将所有日志的字段组合在一起，一条日志不存在的字段的数据可以设置为空，比如启动日志中含有’start’，却没有操作日志中的’actions’,可以将两个字段都放在同一个日志表当中。      全量表：按照时间分区，每一天全部的数据都会同步过来。      增量表：按照时间分区，增量表中每一行的数据为一次变更操作的记录，包括操作类型（insert update），变化后的数据（data字段）和原来旧的数据是什么（old字段）等。        JSON表建立（Hive中）：          语法：        CREATE TABLE table_name(FIELDS VARCHAR(10))ROW FROMAT SERDE 'org.apache.hadoop.hive.serde2.JsonSerDe'STORE AS TEXTFILE;                    SERDE为序列化器和反序列化器的缩写                  在Hive中Hive使用SerDe来读和写表的行                          HDFS files -&gt; InputFileFormat -&gt; &lt;key,value&gt; -&gt; Deserializer -&gt; RowObject              RowObject-&gt; Serializer-&gt; &lt;key,value&gt;-&gt; OutputFileFormat -&gt; HDFS files                                Hive建表底层都会解析成对应的InputFormat，OutputFormat，SerDe                    STORE AS存储格式 默认为TEXTFILE      表内的字段名要与JSON的一级键名对应上才能获取对应字段的数据，但是对应不上也不会报错~                  如果存在嵌套结构的话，可以使用Hive内置的其他复杂类型实现，比如                          STRUCT&lt;name1:type1,name2:type2,name3:type3…&gt;:sturct.name 取值              Array:通过arr[index] 取值              Map&lt;keyType,valueType&gt;:通过map[‘key’] 取值                                复杂数据类型也是可以相互嵌套的，嵌套的字段名也要和键名对应                    在Hive中gzip和bzip2压缩的文本文件可以直接作为TEXTFILE被Hive解析，其他压缩格式可以参考官网文档选用对应压缩格式的建表语句。      项目中建立的都是外部表 CREATE EXTERNAL TABLE;                  内部表由Hive自己管理，而外部表由HDFS管理          删除内部表会直接删除元数据（metadata）及存储数据；删除外部表仅仅会删除元数据，HDFS上的文件并不会被删除；                    "
    },
  
    {
    "title": "Spark Streaming Introduction",
    "url": "/posts/Spark-Streaming-Introduction/",
    "categories": "Bigdata, Spark",
    "tags": "spark",
    "date": "2022-11-25 22:45:00 +0800",
    





    
    "snippet": "Spark Streaming基本介绍Spark StreamingSpark Streaming 用于流式数据的处理。Spark Streaming 支持的数据输入源很多，例如：Kafka、 Flume、Twitter、ZeroMQ 和简单的 TCP 套接字等等。数据输入后可以用 Spark 的高度抽象原语,如：map、reduce、join、window 等进行运算。而结果也能保存在很多...",
    "content": "Spark Streaming基本介绍Spark StreamingSpark Streaming 用于流式数据的处理。Spark Streaming 支持的数据输入源很多，例如：Kafka、 Flume、Twitter、ZeroMQ 和简单的 TCP 套接字等等。数据输入后可以用 Spark 的高度抽象原语,如：map、reduce、join、window 等进行运算。而结果也能保存在很多地方，如 HDFS，数据库等。Spark Streaming数据结构DStream  和 Spark 基于 RDD 的概念很相似，Spark Streaming 使用离散化流(discretized stream)作为抽象表示，叫作 DStream。  DStream 是随时间推移而收到的数据的序列。在内部，每个时间区间收到的数据都作为 RDD 存在，而 DStream 是由这些 RDD 所组成的序列(因此得名“离散化”)。所以简单来讲，DStream 就是对 RDD 在实时数据处理场景的一种封装。"
    },
  
    {
    "title": "Spark Sql Introduction",
    "url": "/posts/Spark-Sql-Introduction/",
    "categories": "Bigdata, Spark",
    "tags": "spark",
    "date": "2022-11-25 22:40:00 +0800",
    





    
    "snippet": "SparkSQL基本介绍Spark SqlSpark SQL 是Spark用于结构化数据处理的Spark模块:  数据兼容方面: SparkSQL 不但兼容 Hive，还可以从 RDD、parquet 文件、JSON 文件中获取数据，未来版本甚至支持获取 RDBMS 数据以及 cassandra 等 NOSQL 数据；  性能优化方面: 除了采取 In-Memory Columnar Sto...",
    "content": "SparkSQL基本介绍Spark SqlSpark SQL 是Spark用于结构化数据处理的Spark模块:  数据兼容方面: SparkSQL 不但兼容 Hive，还可以从 RDD、parquet 文件、JSON 文件中获取数据，未来版本甚至支持获取 RDBMS 数据以及 cassandra 等 NOSQL 数据；  性能优化方面: 除了采取 In-Memory Columnar Storage、byte-code generation 等优化技术外、将会引进 Cost Model 对查询进行动态评估、获取最佳物理计划等等；  组件扩展方面: 无论是 SQL 的语法解析器、分析器还是优化器都可以重新定义，进行扩展。Spark Sql数据结构DataSet  在 Spark 中，DataFrame 是一种以 RDD 为基础的分布式数据集，类似于传统数据库中的二维表格。DataFrame 与 RDD 的主要区别在于，前者带有 schema 元信息，即 DataFrame 所表示的二维表数据集的每一列都带有名称和类型。这使得 Spark SQL 得以洞察更多的结构信息，从而对藏于 DataFrame 背后的数据源以及作用于 DataFrame 之上的变换进行了针对性 的优化，最终达到大幅提升运行时效率的目标。反观 RDD，由于无从得知所存数据元素的具体内部结构，Spark Core 只能在 stage 层面进行简单、通用的流水线优化。  与 Hive 类似，DataFrame 也支持嵌套数据类型（struct、array 和 map）。从 API 易用性的角度上看，DataFrame API 提供的是一套高层的关系操作，比函数式的 RDD API 要更加友好，门槛更低  DataFrame也是懒执行的（就是将某些逻辑延迟到使用时再计算），但是性能比RDD更高，主要原因是执行计划优化，即查询计划通过 Spark catalyst optimiser 进行优化。简而言之， 逻辑查询计划优化就是一个利用基于关系代数的等价变换，将高成本的操作替换为低成本操作的过程。DataFrame  DataSet是分布式数据集合，DataSet 是 Spark 1.6 中添加的一个新抽象，是 DataFrame 的一个扩展。它提供了 RDD 的优势（强类型，使用强大的 lambda 函数的能力）以及 Spark SQL 优化执行引擎的优点。DataSet 也可以使用功能性的转换（操作 map，flatMap，filter 等等）。  DataSet 是 DataFrame API 的一个扩展，是 SparkSQL 最新的数据抽象  用户友好的 API 风格，既具有类型安全检查也具有 DataFrame 的查询优化特性；  用样例类来对 DataSet 中定义数据的结构信息，样例类中每个属性的名称直接映射到 DataSet 中的字段名称；  DataSet 是强类型的。比如可以有 DataSet[Car]，DataSet[Person]。  DataFrame 是 DataSet 的特例，DataFrame=DataSet[Row] ，所以可以通过 as 方法将 DataFrame 转换为 DataSet。Row 是一个类型，跟Car、Person 这些的类型一样，所有的表结构信息都用 Row 来表示。获取数据时需要指定顺序。Spark Sql特点  易整合：整合SQL查询和Spark编程  统一的数据访问：使用相同的方式连接不同的数据源  兼容Hive：可以在已有的仓库上运行SQL或者HiveSQL  标准数据连接：通过JDBC/ODBC连接"
    },
  
    {
    "title": "Spark Deployment Structure Introduction",
    "url": "/posts/Spark-Deployment-Structure-Introduction/",
    "categories": "Bigdata, Spark",
    "tags": "spark",
    "date": "2022-11-25 22:30:00 +0800",
    





    
    "snippet": "Spark IntroductionSpark Summary  Spark 是一种由 Scala 语言开发的快速、通用、可扩展的大数据分析引擎  Spark Core 中提供了 Spark 最基础与最核心的功能  Spark SQL 是 Spark 用来操作结构化数据的组件。通过 Spark SQL，用户可以使用 SQL 或者 Apache Hive 版本的 SQL 方言（HQL）来查询数...",
    "content": "Spark IntroductionSpark Summary  Spark 是一种由 Scala 语言开发的快速、通用、可扩展的大数据分析引擎  Spark Core 中提供了 Spark 最基础与最核心的功能  Spark SQL 是 Spark 用来操作结构化数据的组件。通过 Spark SQL，用户可以使用 SQL 或者 Apache Hive 版本的 SQL 方言（HQL）来查询数据。  Spark Streaming 是 Spark 平台上针对实时数据进行流式计算的组件，提供了丰富的 处理数据流的 API。  Spark 出现的时间相对较晚，并且主要功能主要是用于数据计算， 所以其实 Spark 一直被认为是 Hadoop 框架的升级版。Spark on Hadoop  Spark 就是在传统的 MapReduce 计算框架的基础上，利用其计算过程的优化，从而大大加快了数据分析、挖掘的运行和读写速度，并将计算单元缩小到更适合并行计算和重复使用的 RDD 计算模型。  Spark 和Hadoop 的根本差异是多个作业之间的数据通信问题 : Spark 多个作业之间数据 通信是基于内存，而 Hadoop 是基于磁盘。  Spark Task 的启动时间快。Spark 采用 fork 线程的方式，而 Hadoop 采用创建新的进程 的方式。  Spark 只有在shuffle 的时候将数据写入磁盘，而 Hadoop 中多个 MR 作业之间的数据交 互都要依赖于磁盘交互  Spark 的缓存机制比 HDFS 的缓存机制高效。  但是 Spark 是基于内存的，所以在实际的生产环境中，由于内存的限制，可能会 由于内存资源不够导致 Job 执行失败，此时，MapReduce 其实是一个更好的选择，所以 Spark 并不能完全替代 MR。Spark Core  Spark Core 中提供了 Spark 最基础与最核心的功能，Spark 其他的功能如：Spark SQL， Spark Streaming，GraphX, MLlib 都是在 Spark Core的基础上进行扩展的  Spark SQL：Spark SQL 是 Spark 用来操作结构化数据的组件。通过 Spark SQL，用户可以使用 SQL 或者 Apache Hive 版本的 SQL 方言（HQL）来查询数据。  Spark Streaming：Spark Streaming 是 Spark 平台上针对实时数据进行流式计算的组件，提供了丰富的处理 数据流的 API。  Spark MLlib：MLlib 是 Spark 提供的一个机器学习算法库。MLlib 不仅提供了模型评估、数据导入等额外的功能，还提供了一些更底层的机器学习原语。  Spark GraphX：GraphX 是 Spark 面向图计算提供的框架与算法库。Spark EnvironmentSpark 部署运行环境不同模式最主要的区别还是在指定–master时的运行环境和模式  Local模式：本地环境：不需要其他任何节点资源就可以在本地执行 Spark 代码的环境          提交jar包到本地环境下运行的命令（官方案例中的计算pi）：                                 bin/spark-submit \\   --class org.apache.spark.examples.SparkPi \\   --master local[2] \\   ./examples/jars/spark-examples2.12-3.0.0.jar                                应用参数提交说明：             bin/spark-submit \\  --class \\ --master \\ ...  # other options \\  [application-arguments]                                                   –class\tSpark 程序中包含主函数的类      –master\tSpark 程序运行的模式(环境)\t模式：local[*]、spark://linux1:7077、 Yarn      –executor-memory 1G\t指定每个 executor 可用内存为 1G      –total-executor-cores 2\t指定所有executor使用的cpu核数。 为 2 个      –executor-cores\t指定每个executor使用的cpu核数      application-jar\t打包好的应用 jar，包含依赖。这 个 URL 在集群中全局可见。 比如 hdfs:// 共享存储系统，如果是file://path，那么所有的节点的 path 都包含同样的 jar      application-arguments \t传给 main()方法的参数        Standalone模式：经典的master-slave模式          配置历史服务  history-server            配置高可用HA（多个master）                  停止集群 sbin/stop-all.sh          启动 Zookeeper xstart zk          修改 spark-env.sh 文件添加如下配置                          注释如下内容：                 #SPARK_MASTER_HOST=linux1   #SPARK_MASTER_PORT=7077                                             添加如下内容:                 SPARK_MASTER_WEBUI_PORT=8989  export SPARK_DAEMON_JAVA_OPTS=\"  -Dspark.deploy.recoveryMode=ZOOKEEPER  -Dspark.deploy.zookeeper.url=hadoop102,hadoop103,hadoop104  -Dspark.deploy.zookeeper.dir=/spark\"                                                                           Yarn模式：standalone模式有spark自身提提供计算资源，无需其他框架提供资源，这样降低了和其他第三方框架的耦合性，但是spark本身最主要用途是计算框架，而不是资源调度框架，所以还是用其他更专业的资源调度框架更靠谱。采用yarn进行资源调度。                                  提交应用的时候指定–master参数为yarn            k8s&amp;mesos模式容器化部署：容器管理工具中最为流行的就是 Kubernetes（k8s），而 Spark 也在最近的版本中支持了 k8s 部署模式。    windows模式：不用启用虚拟机直接使用Spark运行架构采用标准的主从结构master-slave  Driver：用于执行Spark中的main方法  Executor：Spark Executor 是集群中运行在工作节点（Worker）中的一个 JVM 进程，是整个集群中 的专门用于计算的节点。资源一般指的是工作节点 Executor 的内存大小和使用的虚拟 CPU 核（Core）数量。                并行和并发：并行为真的含有多个核同时运行，并发为一个核一直切换任务看起来像执行多个任务      Spark 为代表的第三代的计算引擎。第三代计算引擎的特点主要是 Job 内部的 DAG 支持（不跨越 Job），以及实时计算。 第一代为MR引擎要拆分算法，第二代为Tez等支持DAG的框架              Matser&amp;Worker  ApplicationMaster    任务提交流程      Yarn Client模式：          Client 模式将用于监控和调度的 Driver 模块在客户端执行，而不是在 Yarn 中，所以一般用于测试。      Driver 在任务提交的本地机器上运行      Driver 启动后会和 ResourceManager 通讯申请启动 ApplicationMaster      ResourceManager 分配 container，在合适的 NodeManager 上启动 ApplicationMaster，负 责向 ResourceManager 申请 Executor 内存      ResourceManager 接到 ApplicationMaster 的资源申请后会分配 container，然后 ApplicationMaster 在资源分配指定的 NodeManager 上启动 Executor 进程      Executor 进程启动后会向 Driver 反向注册，Executor 全部注册完成后 Driver 开始执行 main 函数      之后执行到 Action 算子时，触发一个 Job，并根据宽依赖开始划分 stage，每个 stage 生 成对应的 TaskSet，之后将 task 分发到各个 Executor 上执行。        Yarn Cluster模式 1.Cluster 模式将用于监控和调度的 Driver 模块启动在 Yarn 集群资源中执行。一般应用于 实际生产环境。          在 YARN Cluster 模式下，任务提交后会和 ResourceManager 通讯申请启动 ApplicationMaster      随后 ResourceManager 分配 container，在合适的 NodeManager 上启动 ApplicationMaster， 此时的 ApplicationMaster 就是 Driver。      Driver 启动后向 ResourceManager 申请 Executor 内存，ResourceManager 接到 ApplicationMaster 的资源申请后会分配 container，然后在合适的 NodeManager 上启动 Executor 进程      Executor进程启动后会向 Driver 反向注册，Executor 全部注册完成后 Driver 开始执行 main函数      之后执行到 Action 算子时，触发一个 Job，并根据宽依赖开始划分 stage，每个 stage 生 成对应的 TaskSet，之后将 task 分发到各个 Executor 上执行。      "
    },
  
    {
    "title": "Spark Data Structure Introduction",
    "url": "/posts/Spark-Data-Structure-Introduction/",
    "categories": "Bigdata, Spark",
    "tags": "spark",
    "date": "2022-11-25 22:30:00 +0800",
    





    
    "snippet": "核心编程：Spark 计算框架为了能够进行高并发和高吞吐的数据处理，封装了三大数据结构，用于处理不同的应用场景。三大数据结构分别是:  累加器：分布式共享只写变量  广播变量：分布式共享只读变量  RDD : 弹性分布式数据集累加器:累加器用来把 Executor 端变量信息聚合到 Driver 端。在 Driver 程序中定义的变量，在 Executor 端的每个 Task 都会得到这个变...",
    "content": "核心编程：Spark 计算框架为了能够进行高并发和高吞吐的数据处理，封装了三大数据结构，用于处理不同的应用场景。三大数据结构分别是:  累加器：分布式共享只写变量  广播变量：分布式共享只读变量  RDD : 弹性分布式数据集累加器:累加器用来把 Executor 端变量信息聚合到 Driver 端。在 Driver 程序中定义的变量，在 Executor 端的每个 Task 都会得到这个变量的一份新的副本，每个 task 更新这些副本的值后，传回 Driver 端进行merge。  merge的原因: spark的分布式计算，会将数据分到不同的Executor上计算完  分布式共享的只写变量          分布式：累加器的值是分布式统计执行的      共享：累加器被多个Executor共享      只写：说明Executor之间累加器是不同互相访问，是读不到的，只有Driver可以读取每个Executor上的累加器        累加器中会出现少加和多加的情况          少加：转换算子中调用了累加器，但是没有执行行动算子，那么累加器不会执行生效      多家：多次调用了行动算子，累加器会多次执行      为了防止少加和多加的情况，一般会把累加器放在行动算子中使用，而不会放在转换算子中      累加器类别  系统累加器：          通过sparkContext获取        var acc1 = sc.longAccumulator(\"acc_name\")sc.doubleAccumulator(\"acc_name\")sc.collectionAccumulator(\"acc_name\")                    使用累加器实例代码:        rdd.foreach(num=&gt;{acc1.sum(num)})                      自定义累加器：          继承AccumulatorV2接口，定义泛型，定义IN累加器输入的数据类型，定义OUT累加器输出的类型                  class MyAccumulator extends AccumulatorV2[IN,OUT]                    重写实现累加器接口中的方法: add      向sc注册累加器          val acc = new MyAccumulator()  sc.register(acc,\"acc_name\")                    使用累加器打印结果          rdd.foreach(num=&gt;{acc.add(num)})  print(acc.value)                    广播变量:广播变量用来高效分发较大的对象。向所有工作节点发送一个较大的只读值，以供一个或多个 Spark 操作使用。比如，如果你的应用需要向所有节点发送一个较大的只读查询表， 广播变量用起来都很顺手。在多个并行操作中使用同一个变量，Spark会为每个任务分别发送（闭包数据都是以Task为单位发送的），会导致在Executor中包含同样的一份数据导致冗余，占用大量的内存。广播变量就是将数据放在Executor的内存当中，而不是放在每一个task中，从而达到共享的目的。这份数据只读，保证线程安全。  分布式共享只读变量  声明广播变量    val broadcast = sc.broadcast(list)      RDD:全称为 resilient distributed dataset 弹性分布式数据集, Spark中最基本的数据处理模型。代码中是一个抽象类，它代表一个弹性的、不可变、可分区、里面的元素可并行计算的集合。每个RDD都是一个计算单元，将多个RDD进行关联组成一个完整的需求计算逻辑。弹性分布式数据集  弹性          存储的弹性：内存与磁盘的自动切换；      容错的弹性：数据丢失可以自动恢复；      计算的弹性：计算出错重试机制；      分片的弹性：可根据需要重新分片。        分布式：数据存储在大数据集群不同节点上  数据集：RDD 封装了计算逻辑，并不保存数据  数据抽象：RDD 是一个抽象类，需要子类具体实现  不可变：RDD 封装了计算逻辑，是不可以改变的，想要改变，只能产生新的 RDD，在新的 RDD 里面封装计算逻辑  可分区、并行计算核心属性  分区列表:list of partitions          RDD数据结构中存在分区列表，用于执行任务时并行计算，是实现分布式计算的重要属性        分区计算函数:function for computing each split          compute()      Spark计算的时候使用分区函数对每一个分区进行计算      一个分区都会有一个task，但是所有分区的计算逻辑和方式相同        RDD之间的依赖关系:dependencies on each RDD          getDependencies()      RDD是计算模型的封装，当需求当中需要将多个计算模型进行组合时，需要将多个RDD建立依赖关系        分区器(可选):a partitioner for key-value RDDs          partitioner:Option[Partitioner]      将数据进行分区处理，逻辑可以自己定义（要求数据为KV类型数据）        首选位置(可选 ):preferred locations 计算数据时，可以根据计算节点的状态选择不同的节点位置进行计算（比如选择节点距离最近的服务器来运行计算服务等）          getPreferredLocations      判断计算发送哪个节点效率最优，移动数据不如移动计算      "
    },
  
    {
    "title": "Git",
    "url": "/posts/Git/",
    "categories": "Program, Java",
    "tags": "Java",
    "date": "2022-11-22 22:30:00 +0800",
    





    
    "snippet": "Git安装Linux  包管理器安装    # Ubuntu 10.10 or uppersudo aptitude install gitsudo aptitude install git-doc git-svn git-email gitk  # RHEL CentOsyum install gityum install git-svn git-email gitk        源码安...",
    "content": "Git安装Linux  包管理器安装    # Ubuntu 10.10 or uppersudo aptitude install gitsudo aptitude install git-doc git-svn git-email gitk  # RHEL CentOsyum install gityum install git-svn git-email gitk        源码安装    # decompress git pkgtar -jxvf git-xxxxcd git-xxxx  # install git to /usr/local/binmake prefix = /usr/local allsudo make prefix = /usr/local install  # install git docsmake prefix = /usr/local doc infosudo make prefix = /usr/local install-doc install-html install-info        命令补全    cp contrib/completion/git-completion.bash /etc/bash_completion.d/# current shell is valid. /etc/bash_completion  # auto load bash_completion# add lines following to \"~/.bash_profile\" or \"/etc/bashrc\"if [ -f /etc/bash_completion]; then  . /etc/bash_completionfi      Windows      下载安装包解压即可        GIT UI: TortoiseGit  Git基础配置配置级别优先级：  项目配置 &gt; 用户配置 &gt; 系统配置  系统配置：    # 存放在git安装目录下 %Git%/etc/gitconfiggit config --system core.autocrlf xxx        用户配置    # 存放在用户目录下 ~/.gitconfiggit config --global user.name xxx        项目配置    # 当前仓库配置文件 .git/configgit config --local remote.origin.url xxx      基本配置  配置个人身份，作为用户配置    git config --global user.name \"breelynn\"git config --global user.email \"breelynn@xxx.com\"        文本换行符    # Git 提交会将行结束换行符CRLF转换为LF，如果在windows系统下打开core.autocrlfgit config --global core.autocrlf true  # Linux/Mac使用LF作为行结束符，当CRLF引入时进行修正，可以将core.autocrlf设置为input使Git将CRLF转换为LFgit config --global core.autocrlf input  # windows开发且不涉及其他跨平台可以关闭该功能，把回车符记录在repo中git config --global core.autocrlf false        文本编码配置    # 中文编码支持git config --global gui.encoding utf-8git config --global i18n.commitencoding utf-8git config --global i18n.logoutputencoding utf-8  # 显示中文路径git config --global core.quotepath false        与服务器的认证配置          http / https 协议认证          # 设置口令缓存    git config --global credential.helpe store  # 添加HTTPS证书    git config http.sslverify false                    ssh 协议认证          # Git Bash生成公钥后保存在代码平台中    ssh-keygen -t rsa -C \"breelynn@xxx.com\"                Git基本命令            "
    },
  
    {
    "title": "Update logs",
    "url": "/posts/Update-logs/",
    "categories": "Logs, Update",
    "tags": "update, logs",
    "date": "2022-11-20 20:00:00 +0800",
    





    
    "snippet": "Update date: 2023-01-29界面展示  home:    Update logs    1️⃣ 启用根据post类别和标签进行跳转和分类查询的功能, 包括根据归档和标签进行检索的查询方式.2️⃣ 区分post time和modified time; 增加了timeline的tabs, 可以使用timeline以post time作为顺序查看, 也可根据主页以modified...",
    "content": "Update date: 2023-01-29界面展示  home:    Update logs    1️⃣ 启用根据post类别和标签进行跳转和分类查询的功能, 包括根据归档和标签进行检索的查询方式.2️⃣ 区分post time和modified time; 增加了timeline的tabs, 可以使用timeline以post time作为顺序查看, 也可根据主页以modified time作为顺序查看.3️⃣ 增加Plans tabs, 用于归纳未来计划. 4️⃣ 每一页的分页数量从5增加到8, 更好的支持竖屏模式.Update date: 2022-11-27界面展示  home:  post:Update logs  1️⃣ contact info增加了b站的联系方式, 点击右下角即可跳转到b站个人主页.修改了contact info中个人邮箱的获取逻辑, 如果电脑中有outlook会直接打开outlook并获取联系邮箱.修改了contactinfo中个人邮箱的获取逻辑, 如果电脑中有outlook会直接打开outlook并获取联系邮箱.2️⃣ posts sharing增加了QQ,微信,b站,微博,linkedin的sharing方式, 保留twitter; 其中qq和微信不会直接跳转到分享页面, 而是通过生成分享二维码的方式进行取代.3️⃣ 调整了两种颜色主题的全局表现, 现在默认界面会根据打开平台进行默认设置(比如移动端使用了暗色主题,则会自动打开黑暗模式), pc端目前是默认亮色模式4️⃣ 增加了top bar中searching的功能, 现在可以通过检索关键字来筛选对应的文章了.5️⃣ 增加了右侧update list和tags trending的显示界面, 可以通过update list来获取最新发布的博客了.6️⃣ 增加了打开博客后右侧显示当前文章内标题的功能,可以点击右侧content中的文章标题进行定位跳转.(此外,标题也与文章内的标题进行了连接,可以直接点击文章内标题进行界面矫正)Known issues  🔍 目前移动端可能会因为横竖屏的原因出现页面展示的问题, 不过并不影响使用,甚至还挺好的…🐛 暗色主题表现在不同平台可能有会bug,比如帖子文本内容也被渲染成了背景色,但是文本确实展示了,只不过和背景融为一体了…🐛 search中右侧清空按钮存在不生效的问题,目前手动按delete而不能点击按钮一键清空Others  🔒 暂时关闭根据post类别和标签进行跳转和分类查询的功能,目前使用相关会强制转到404页面.但不影响top bar search中根据文章内容检索的功能~❗ 关闭根据post发布日期排序的功能,主页展示出来的已经是排序后的结果了.Update date: 2022-11-20界面展示  home:  post:"
    },
  
    {
    "title": "Some tips to beautify markdown",
    "url": "/posts/Some-tips-to-beautify-markdown/",
    "categories": "Markdown, Edit",
    "tags": "markdown",
    "date": "2022-11-19 20:00:00 +0800",
    





    
    "snippet": "This post is to show how sites renders markdown file syntax and beatify it to beautify it to be more in line with viewing blog.Dividing lineYou can write --- to your md to show a dividing line and ...",
    "content": "This post is to show how sites renders markdown file syntax and beatify it to beautify it to be more in line with viewing blog.Dividing lineYou can write --- to your md to show a dividing line and the line is as followed:ListsOrdered listYou can use code like this to specify an ordered list1. Firstly2. Secondly3. Thirdly  Firstly  Secondly  ThirdlyUnordered listYou can use code like this to specify an unordered list- Chapter    + Section        * Paragraph  Chapter          Section                  Paragraph                    ToDo listYou can use code like this to specify a to-do list- [ ] Job    + [x] Step 1    + [x] Step 2    + [ ] Step 3  Job          Step 1      Step 2      Step 3      Description listYou can use code like this to specify a to-do listDogs: I like samoyed~Cats: Prefer dogs, don't you?  Dogs  I like samoyed~  Cats  Prefer dogs, don’t you?Block QuoteInline block quoteThe is an example of `Inline block quote`.This is an example of Inline block quote.Simplest block quote&gt; This line shows the _block quote_.  This line shows the block quote.Prompts block quote&gt; An example showing the `type` type prompt.({: .prompt-tip }|{: .prompt-info }|{: .prompt-warning }|{: .prompt-danger })  An example showing the tip type prompt.  An example showing the info type prompt.  An example showing the warning type prompt.  An example showing the danger type prompt.Tables block quoteThe code may be a little complicated:| Primary Key                  | Color                     | Size  ||:-----------------------------|:--------------------------|------:|| Samoyed                      | White                     | Large || Golden retriever             | Golden Yellow             | Large || Border collie                | White and black combined  | Large |            Primary Key      Color      Size                  Samoyed      White      Large              Golden retriever      Golden Yellow      Large              Border collie      White and black combined      Large      Common code blockUse double \"``”` to surround your codesThis is a common code snippet, without syntax highlight and line number.Specific LanguagesSpecific the language name after the first \"``”`Java:  class Main {    public static void main(String[] args) {      System.out.println(\"hello world\");    }  }Shell:if [ $1 -ne 0 ]; then    echo \"The command was not successful.\";fi;SQL:SELECT *FROM parquet.`/PATH/TO/PARQUET_FILE.parquet`WHERE PRIMARY_KEY=\"samoyed\";FootnoteUse [^fn-nt-?] to locate a footnote, and the ? means the serial number.Click the hook will locate the first footnote1, and here is second footnote2.LinksUse &lt;&gt; to include a linkhttp://127.0.0.1:80FilepathSimple filename formatHere is the `/simple/filename/file`{: .filepath}.Here is the /simple/filename/file.Specific filename format@import  \"colors/light-typography\",  \"colors/dark-typography\"Reverse Footnote            The first footnote source &#8617;              The second footnote source &#8617;      "
    },
  
    {
    "title": "Use markdown to blog",
    "url": "/posts/Use-markdown-to-blog/",
    "categories": "Markdown, Edit",
    "tags": "markdown",
    "date": "2022-11-19 00:00:00 +0800",
    





    
    "snippet": "Naming and PathCreate a new file named YYYY-MM-DD-TITLE.EXTENSION and put it in the _posts of the root directory. Please note that the EXTENSION must be one of md and markdown. If you want to save ...",
    "content": "Naming and PathCreate a new file named YYYY-MM-DD-TITLE.EXTENSION and put it in the _posts of the root directory. Please note that the EXTENSION must be one of md and markdown. If you want to save time of creating files, please consider using the plugin Jekyll-Compose to accomplish this.Front MatterBasically, you need to fill the Front Matter as below at the top of the post:---title: TITLEdate: YYYY-MM-DD HH:MM:SS +/-TTTTcategories: [TOP_CATEGORIE, SUB_CATEGORIE]tags: [TAG]     # TAG names should always be lowercase---  The md file in ‘_post’ directory has been set to post by default, so there is no need to add the variable layout in the Front Matter block.Timezone of DateIn order to accurately record the release date of a post, you should not only set up the timezone of _config.yml but also provide the post’s timezone in variable date of its Front Matter block. Format: +/-TTTT, e.g. +0800.Categories and TagsThe categories of each post are designed to contain up to two elements, and the number of elements in tags can be zero to infinity. For instance:---categories: [Dogs, Samoyed]tags: [samoyed]---Author InformationThe author information of the post usually does not need to be filled in the Front Matter , they will be obtained from variables social.name and the first entry of social.links of the configuration file by default. But you can also override it as follows:Adding author information in _data/authors.yml.&lt;author_id&gt;:  name: &lt;full name&gt;  twitter: &lt;twitter_of_author&gt;  url: &lt;homepage_of_author&gt;And then use author to specify a single entry or authors to specify multiple entries:---author: &lt;author_id&gt;                     # for single entry# orauthors: [&lt;author1_id&gt;, &lt;author2_id&gt;]   # for multiple entries---Having said that, the key author can also identify multiple entries.  The benefit of reading the author information from the file _data/authors.yml is that the page will have the meta tag twitter:creator, which enriches the Twitter Cards and is good for SEO.Table of ContentsBy default, the Table of Contents (TOC) is displayed on the right panel of the post. If you want to turn it off globally, go to _config.yml and set the value of variable toc to false. If you want to turn off TOC for a specific post, add the following to the post’s Front Matter:---toc: false---CommentsThe global switch of comments is defined by variable comments.active in the file _config.yml. After selecting a comment system for this variable, comments will be turned on for all posts.If you want to close the comment for a specific post, add the following to the Front Matter of the post:---comments: false---MathematicsFor website performance reasons, the mathematical feature won’t be loaded by default. But it can be enabled by:---math: true---MermaidMermaid is a great diagrams generation tool. To enable it on your post, add the following to the YAML block:---mermaid: true---Then you can use it like other markdown languages: surround the graph code with ```mermaid and ```.ImagesCaptionAdd italics to the next line of an image，then it will become the caption and appear at the bottom of the image:![img-description](/path/to/image)_Image Caption_SizeIn order to prevent the page content layout from shifting when the image is loaded, we should set the width and height for each image:![Desktop View](/assets/img/sample/mockup.png){: width=\"700\" height=\"400\" }Starting from Chirpy v5.0.0, height and width support abbreviations (height → h, width → w). The following example has the same effect as the above:![Desktop View](/assets/img/sample/mockup.png){: w=\"700\" h=\"400\" }PositionBy default, the image is centered, but you can specify the position by using one of the classes normal, left, and right.  Once the position is specified, the image caption should not be added.      Normal position    Image will be left aligned in below sample:    ![Desktop View](/assets/img/sample/mockup.png){: .normal }            Float to the left    ![Desktop View](/assets/img/sample/mockup.png){: .left }            Float to the right    ![Desktop View](/assets/img/sample/mockup.png){: .right }      ShadowThe screenshots of the program window can be considered to show the shadow effect, and the shadow will be visible in the light mode:![Desktop View](/assets/img/sample/mockup.png){: .shadow }CDN URLIf you host the images on the CDN, you can save the time of repeatedly writing the CDN URL by assigning the variable img_cdn of _config.yml file:img_cdn: https://cdn.comOnce img_cdn is assigned, the CDN URL will be added to the path of all images (images of site avatar and posts) starting with /.For instance, when using images:![The flower](/path/to/flower.png)The parsing result will automatically add the CDN prefix https://cdn.com before the image path:&lt;img src=\"https://cdn.com/path/to/flower.png\" alt=\"The flower\"&gt;Image PathWhen a post contains many images, it will be a time-consuming task to repeatedly define the path of the images. To solve this, we can define this path in the YAML block of the post:---img_path: /img/path/---And then, the image source of Markdown can write the file name directly:![The flower](flower.png)The output will be:&lt;img src=\"/img/path/flower.png\" alt=\"The flower\"&gt;Preview ImageIf you want to add an image to the top of the post contents, specify the attribute path, width, height, and alt for the image:---image:  path: /path/to/image  width: 1000   # in pixels  height: 400   # in pixels  alt: image alternative text---Except for alt, all other options are necessary, especially the width and height, which are related to user experience and web page loading performance. The above section “Size” also mentions this.Starting from Chirpy v5.0.0, the attributes height and width can be abbreviated: height → h, width → w. In addition, the img_path can also be passed to the preview image, that is, when it has been set, the  attribute path only needs the image file name.For simple use, you can also just use image to define the path.image: /path/to/imagePinned PostsYou can pin one or more posts to the top of the home page, and the fixed posts are sorted in reverse order according to their release date. Enable by:---pin: true---PromptsThere are several types of prompts: tip, info, warning, and danger. They can be generated by adding the class prompt-{type} to the blockquote. For example, define a prompt of type info as follows:&gt; Example line for prompt.{: .prompt-info }SyntaxInline Code`inline code part`Filepath Hightlight`/path/to/a/file.extend`{: .filepath}Code BlockMarkdown symbols ``` can easily create a code block as follows:```This is a plaintext code snippet.```Specifying LanguageUsing ```{language} you will get a code block with syntax highlight:```yamlkey: value```  The Jekyll tag { % highlight % } is not compatible with this theme. But you should use gem 'rouge' to install dependency.Line NumberBy default, all languages except plaintext, console, and terminal will display line numbers. When you want to hide the line number of a code block, add the class nolineno to it:```shellecho 'No more line numbers!'```{: .nolineno }Specifying the FilenameYou may have noticed that the code language will be displayed at the top of the code block. If you want to replace it with the file name, you can add the attribute file to achieve this:```shell# content```{: file=\"path/to/file\" }"
    }
  
]

